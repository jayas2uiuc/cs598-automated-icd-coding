{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152dae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3684e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nRecords = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa36968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(text):\n",
    "    remove = string.punctuation.replace(\"'\", \"\") \n",
    "    pattern = r\"[{}]\".format(remove) \n",
    "    return re.sub(pattern, \" \", text) \n",
    "\n",
    "def replaceDigitsWithd(text):\n",
    "    return re.sub('\\d', 'd', text)\n",
    "\n",
    "def preprocessText(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = removePunctuations(text)\n",
    "    text = replaceDigitsWithd(text)\n",
    "    output = text.split(' ')\n",
    "    output = list(filter(lambda a: a != '', output))\n",
    "    return output\n",
    "\n",
    "def getWordCounts(data):\n",
    "    combinedList = list(itertools.chain.from_iterable(data))\n",
    "    countDict = dict(Counter(combinedList))\n",
    "    return countDict\n",
    "\n",
    "def findCorrectSpelling(inputWord, countDict, minFreq):  \n",
    "    minDistance = 1e10\n",
    "    correctSpelling = inputWord\n",
    "    for word in countDict.keys():        \n",
    "        if inputWord != word:\n",
    "            distance = nltk.edit_distance(word, inputWord)\n",
    "            if distance < minDistance:\n",
    "                minDistance = distance\n",
    "                correctSpelling = word\n",
    "    return correctSpelling\n",
    "\n",
    "def getSpellingCorrections(countDict, minFreq = 5):\n",
    "    corrections = {}\n",
    "    words = countDict.keys()\n",
    "    for word in words:\n",
    "        if countDict[word] < minFreq:\n",
    "            corrections[word] = findCorrectSpelling(word, countDict, minFreq)\n",
    "    return corrections\n",
    "\n",
    "def preprocess(inputdata):\n",
    "    data = []\n",
    "\n",
    "    for text in inputdata: \n",
    "        data.append(preprocessText(text))\n",
    "         \n",
    "    processedData = spellify(data, minFreq=5)\n",
    "    return processedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705559bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('./cleanData.pkl')\n",
    "dataset = dataset[:nRecords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f733f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:21<00:00, 28.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Correct spellings\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "tokenFreq = Counter(itertools.chain.from_iterable(dataset['PREPROCESSED_TEXT']))\n",
    "wrongSpellings = [] \n",
    "for token in tokenFreq:\n",
    "    if tokenFreq[token] <= 5:\n",
    "        wrongSpellings.append(token)\n",
    "        \n",
    "word2idx = {k: i+1 for i, k in enumerate(tokenFreq.keys())}\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def spellify(text):\n",
    "    newRecord = []\n",
    "    for word in text:\n",
    "        if word in wrongSpellings:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if len(suggestions) > 0:\n",
    "                newRecord.append(suggestions[0].term)\n",
    "        else:\n",
    "            newRecord.append(word)\n",
    "    return newRecord\n",
    "\n",
    "dataset['PREPROCESSED_TEXT'] = dataset['PREPROCESSED_TEXT'].progress_apply(spellify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68626db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "dataset['ICD9_CODE'] = dataset.ICD9_CODE.apply(lambda x: literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342ccdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cleanData-' + str(nRecords) + '.pkl', 'wb') as handle:\n",
    "    pickle.dump(dataset, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
